1) Для запуска проекта необходимо сначала запустить Docker Compose:
```shell
docker compose up --build
```

2) После этого Docker сделает следующее:
	- проинициализирует модель "снежинка" в **PostgreSQL**;
	- построчно считает данные из исходных таблиц и отправит их в **Kafka** в формате `.json`;
	- поднимет среду для запуска **Flink** и установит все необходимые утилиты.

3) Далее необходимо в новом окне терминала выполнить:
```shell
docker exec -it jobmanager python3 /app/flink_job.py
```

4) Это запустит **Flink job**, который преобразует данные из **Kafka** в модель "снежинка" и положит преобразованные таблицы в **PostgreSQL**.
5) Наконец, можно подключиться к созданной базе данных из любого инструмента управления СУБД и убедиться, что данные были успешно сохранены в базу.
6) Основной код проекта лежит в директориях `producer/` и `flink/`:
```
├── flink
│   ├── Dockerfile
│   ├── flink_job.py
│   └── requirements.txt
└── producer
    ├── Dockerfile
    ├── requirements.txt
    └── send_to_kafka.py
```

7) `producer` автоматически запускается при запуске Docker Compose. Он построчно считывает данные из исходных таблиц, преобразовывает их в формат `.json` и кладет в топик **Kafka**.
8) В `flink/flink_job.py` реализовано потоковое считывание данных из **Kafka**, преобразование их в модель "снежинка" и отправка в **PostgreSQL**.
